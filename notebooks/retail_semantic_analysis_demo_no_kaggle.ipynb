{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Semantic Analysis Demo (No Kaggle Required)\n",
    "This notebook demonstrates the complete pipeline for retail semantic analysis using sample data that doesn't require external downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully!\n",
      "Working directory: /Users/mayankdw/retail_semantic_analysis/notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "from data_loader import RetailDataLoader\n",
    "from preprocessor import RetailTextPreprocessor\n",
    "from sentiment_analyzer import RetailSentimentAnalyzer\n",
    "from topic_modeling import RetailTopicModeler\n",
    "from visualizations import RetailVisualizationGenerator\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords', download_dir='/Users/mayankdw/nltk_data')\n",
    "\n",
    "# import os\n",
    "# os.environ['NLTK_DATA'] = '/Users/mayankdw/nltk_data'\n",
    "\n",
    "print(\"All modules imported successfully!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation (Using Sample Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sample dataset...\n",
      "Dataset loaded with 2000 reviews\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   review_text       2000 non-null   object\n",
      " 1   sentiment         2000 non-null   object\n",
      " 2   rating            2000 non-null   int64 \n",
      " 3   product_category  2000 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 62.6+ KB\n",
      "None\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>rating</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Outstanding customer service and product quality.</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Would not recommend. Poor customer service.</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perfect item, exactly as described. Five stars!</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>Clothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Outstanding customer service and product quality.</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perfect item, exactly as described. Five stars!</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>Clothing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text sentiment  rating  \\\n",
       "0  Outstanding customer service and product quality.  positive       4   \n",
       "1        Would not recommend. Poor customer service.  negative       1   \n",
       "2    Perfect item, exactly as described. Five stars!  positive       4   \n",
       "3  Outstanding customer service and product quality.  positive       5   \n",
       "4    Perfect item, exactly as described. Five stars!  positive       5   \n",
       "\n",
       "  product_category  \n",
       "0             Home  \n",
       "1      Electronics  \n",
       "2         Clothing  \n",
       "3             Home  \n",
       "4         Clothing  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset statistics:\n",
      "Categories: {'Clothing': 538, 'Electronics': 495, 'Books': 486, 'Home': 481}\n",
      "Rating distribution: {1: 407, 2: 407, 4: 584, 5: 602}\n",
      "Sentiment distribution: {'positive': 1186, 'negative': 814}\n"
     ]
    }
   ],
   "source": [
    "# Initialize data loader\n",
    "loader = RetailDataLoader()\n",
    "\n",
    "# Create a more realistic sample dataset for demonstration\n",
    "print(\"Creating sample dataset...\")\n",
    "df = loader.create_sample_dataset(size=2000)\n",
    "\n",
    "print(f\"Dataset loaded with {len(df)} reviews\")\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nDataset statistics:\")\n",
    "print(f\"Categories: {df['product_category'].value_counts().to_dict()}\")\n",
    "print(f\"Rating distribution: {df['rating'].value_counts().sort_index().to_dict()}\")\n",
    "print(f\"Sentiment distribution: {df['sentiment'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Sample Data Creation\n",
    "Let's create more realistic and diverse sample data for better demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced dataset created with 2000 reviews\n",
      "\n",
      "Enhanced dataset statistics:\n",
      "Categories: {'Electronics': 699, 'Clothing': 511, 'Books': 412, 'Home & Kitchen': 378}\n",
      "Rating distribution: {1: 282, 2: 197, 3: 292, 4: 376, 5: 853}\n",
      "Sentiment distribution: {'positive': 1229, 'negative': 479, 'neutral': 292}\n",
      "Date range: 2023-07-09 22:41:44.591750 to 2025-07-07 22:41:44.620479\n"
     ]
    }
   ],
   "source": [
    "# Create enhanced sample data with more realistic reviews\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enhanced review templates\n",
    "positive_reviews = [\n",
    "    \"This product exceeded my expectations! The quality is outstanding and delivery was fast.\",\n",
    "    \"Excellent value for money. The material feels premium and the design is beautiful.\",\n",
    "    \"Perfect fit and great functionality. I would definitely recommend this to others.\",\n",
    "    \"Amazing customer service and quick response. The product works exactly as described.\",\n",
    "    \"Love the innovative features and user-friendly design. Five stars!\",\n",
    "    \"Great packaging and the item arrived in perfect condition. Very satisfied.\",\n",
    "    \"This is exactly what I was looking for. Good quality and reasonable price.\",\n",
    "    \"Impressive build quality and attention to detail. Worth every penny.\",\n",
    "    \"Fast shipping and excellent product. Will definitely buy from this seller again.\",\n",
    "    \"Outstanding performance and reliability. Highly recommended for anyone.\"\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"Poor quality materials and the product broke after just a few days of use.\",\n",
    "    \"Very disappointed with this purchase. Not as described and overpriced.\",\n",
    "    \"Terrible customer service and slow shipping. The item was damaged on arrival.\",\n",
    "    \"Cheap construction and doesn't work properly. Waste of money.\",\n",
    "    \"The fit is completely wrong and the material feels flimsy.\",\n",
    "    \"Misleading product description. What I received was nothing like the pictures.\",\n",
    "    \"Took forever to arrive and when it did, it was defective.\",\n",
    "    \"Not worth the price at all. Very poor quality and design.\",\n",
    "    \"Packaging was terrible and the product was damaged. Very unsatisfied.\",\n",
    "    \"Would not recommend this product to anyone. Save your money.\"\n",
    "]\n",
    "\n",
    "neutral_reviews = [\n",
    "    \"The product is okay, nothing special but does what it's supposed to do.\",\n",
    "    \"Average quality for the price. Could be better but not terrible.\",\n",
    "    \"It's fine, meets basic expectations but nothing extraordinary.\",\n",
    "    \"Decent product with some pros and cons. Mixed feelings about it.\",\n",
    "    \"Works as expected, though there are some minor issues.\",\n",
    "    \"The quality is acceptable for this price range. Nothing more, nothing less.\",\n",
    "    \"Standard product with average performance. Does the job.\",\n",
    "    \"It's an okay purchase. Not great but not bad either.\",\n",
    "    \"Functional but could use some improvements in design.\",\n",
    "    \"Fair value for money. Some good features, some not so good.\"\n",
    "]\n",
    "\n",
    "# Create enhanced dataset\n",
    "reviews = []\n",
    "sentiments = []\n",
    "ratings = []\n",
    "categories = []\n",
    "dates = []\n",
    "\n",
    "# Generate more realistic distribution\n",
    "for i in range(2000):\n",
    "    # Generate date within last 2 years\n",
    "    start_date = datetime.now() - timedelta(days=730)\n",
    "    random_days = np.random.randint(0, 730)\n",
    "    review_date = start_date + timedelta(days=random_days)\n",
    "    \n",
    "    # Determine sentiment with realistic distribution\n",
    "    rand = np.random.random()\n",
    "    if rand < 0.6:  # 60% positive\n",
    "        review = np.random.choice(positive_reviews)\n",
    "        sentiment = 'positive'\n",
    "        rating = np.random.choice([4, 5], p=[0.3, 0.7])\n",
    "    elif rand < 0.85:  # 25% negative\n",
    "        review = np.random.choice(negative_reviews)\n",
    "        sentiment = 'negative'\n",
    "        rating = np.random.choice([1, 2], p=[0.6, 0.4])\n",
    "    else:  # 15% neutral\n",
    "        review = np.random.choice(neutral_reviews)\n",
    "        sentiment = 'neutral'\n",
    "        rating = 3\n",
    "    \n",
    "    category = np.random.choice(['Electronics', 'Clothing', 'Books', 'Home & Kitchen'], \n",
    "                               p=[0.35, 0.25, 0.20, 0.20])\n",
    "    \n",
    "    reviews.append(review)\n",
    "    sentiments.append(sentiment)\n",
    "    ratings.append(rating)\n",
    "    categories.append(category)\n",
    "    dates.append(review_date)\n",
    "\n",
    "# Create enhanced DataFrame\n",
    "df_enhanced = pd.DataFrame({\n",
    "    'review_text': reviews,\n",
    "    'sentiment': sentiments,\n",
    "    'rating': ratings,\n",
    "    'product_category': categories,\n",
    "    'review_date': dates\n",
    "})\n",
    "\n",
    "print(f\"Enhanced dataset created with {len(df_enhanced)} reviews\")\n",
    "print(\"\\nEnhanced dataset statistics:\")\n",
    "print(f\"Categories: {df_enhanced['product_category'].value_counts().to_dict()}\")\n",
    "print(f\"Rating distribution: {df_enhanced['rating'].value_counts().sort_index().to_dict()}\")\n",
    "print(f\"Sentiment distribution: {df_enhanced['sentiment'].value_counts().to_dict()}\")\n",
    "print(f\"Date range: {df_enhanced['review_date'].min()} to {df_enhanced['review_date'].max()}\")\n",
    "\n",
    "# Use enhanced dataset\n",
    "df = df_enhanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing text preprocessor...\n",
      "Preprocessing text data...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/Users/mayankdw/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/share/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Preprocess the dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing text data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m df_processed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed dataset has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_processed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mText statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/retail_semantic_analysis/notebooks/../src/preprocessor.py:273\u001b[0m, in \u001b[0;36mRetailTextPreprocessor.preprocess_dataframe\u001b[0;34m(self, df, text_column, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m df_copy \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m df_copy[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_clean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Remove empty reviews after preprocessing\u001b[39;00m\n\u001b[1;32m    278\u001b[0m df_copy \u001b[38;5;241m=\u001b[39m df_copy[df_copy[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_clean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py:4928\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4800\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4801\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4802\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4807\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4808\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4809\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4810\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4811\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4926\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4927\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4928\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4935\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/apply.py:1422\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/apply.py:1502\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1502\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/base.py:925\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mpandas/_libs/lib.pyx:2999\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/retail_semantic_analysis/notebooks/../src/preprocessor.py:274\u001b[0m, in \u001b[0;36mRetailTextPreprocessor.preprocess_dataframe.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    270\u001b[0m df_copy \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[1;32m    273\u001b[0m df_copy[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_clean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_copy[text_column]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m )\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Remove empty reviews after preprocessing\u001b[39;00m\n\u001b[1;32m    278\u001b[0m df_copy \u001b[38;5;241m=\u001b[39m df_copy[df_copy[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_clean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/retail_semantic_analysis/notebooks/../src/preprocessor.py:252\u001b[0m, in \u001b[0;36mRetailTextPreprocessor.preprocess_text\u001b[0;34m(self, text, remove_stopwords, lemmatize, remove_punct, expand_contractions, correct_spelling)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# Lemmatize\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lemmatize:\n\u001b[0;32m--> 252\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/retail_semantic_analysis/notebooks/../src/preprocessor.py:145\u001b[0m, in \u001b[0;36mRetailTextPreprocessor.lemmatize_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 145\u001b[0m         pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_wordnet_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m         lemmatized\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token, pos))\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized)\n",
      "File \u001b[0;32m~/retail_semantic_analysis/notebooks/../src/preprocessor.py:121\u001b[0m, in \u001b[0;36mRetailTextPreprocessor.get_wordnet_pos\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_wordnet_pos\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Map POS tag to first character lemmatizer accepts\"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     tag \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m    122\u001b[0m     tag_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADJ,\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mNOUN,\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mVERB,\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADV\n\u001b[1;32m    127\u001b[0m     }\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tag_dict\u001b[38;5;241m.\u001b[39mget(tag, wordnet\u001b[38;5;241m.\u001b[39mNOUN)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tag/__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tag/__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tag/perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/tag/perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/Users/mayankdw/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/share/nltk_data'\n    - '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessor\n",
    "print(\"Initializing text preprocessor...\")\n",
    "preprocessor = RetailTextPreprocessor(download_nltk=True)\n",
    "\n",
    "# Preprocess the dataset\n",
    "print(\"Preprocessing text data...\")\n",
    "df_processed = preprocessor.preprocess_dataframe(df, text_column='review_text')\n",
    "\n",
    "print(f\"Processed dataset has {len(df_processed)} reviews\")\n",
    "print(\"\\nText statistics:\")\n",
    "stats = preprocessor.get_text_statistics(df_processed)\n",
    "display(stats)\n",
    "\n",
    "# Show example of preprocessing\n",
    "print(\"\\nExample of preprocessing:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Original: {df['review_text'].iloc[i]}\")\n",
    "    print(f\"Processed: {df_processed['review_text_clean'].iloc[i]}\")\n",
    "    print(f\"Word count: {df_processed['review_text_word_count'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzer\n",
    "print(\"Initializing sentiment analyzer...\")\n",
    "sentiment_analyzer = RetailSentimentAnalyzer(model_type='textblob')\n",
    "\n",
    "# Analyze sentiment\n",
    "print(\"Analyzing sentiment...\")\n",
    "df_sentiment = sentiment_analyzer.analyze_dataframe(df_processed)\n",
    "\n",
    "print(\"Sentiment analysis completed!\")\n",
    "print(\"\\nSentiment distribution from analysis:\")\n",
    "sentiment_dist = sentiment_analyzer.get_sentiment_distribution(df_sentiment)\n",
    "display(sentiment_dist)\n",
    "\n",
    "# Compare with original labels\n",
    "print(\"\\nComparison with original sentiment labels:\")\n",
    "comparison = pd.crosstab(df_sentiment['sentiment'], df_sentiment['sentiment_sentiment'], \n",
    "                        margins=True, normalize='columns')\n",
    "display(comparison)\n",
    "\n",
    "# Show example results\n",
    "print(\"\\nExample sentiment analysis results:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Text: {df_sentiment['review_text_clean'].iloc[i][:80]}...\")\n",
    "    print(f\"Original Sentiment: {df_sentiment['sentiment'].iloc[i]}\")\n",
    "    print(f\"Predicted Sentiment: {df_sentiment['sentiment_sentiment'].iloc[i]}\")\n",
    "    print(f\"Confidence: {df_sentiment['sentiment_confidence'].iloc[i]:.3f}\")\n",
    "    if 'sentiment_polarity' in df_sentiment.columns:\n",
    "        print(f\"Polarity: {df_sentiment['sentiment_polarity'].iloc[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize topic modeler\n",
    "print(\"Initializing topic modeler...\")\n",
    "topic_modeler = RetailTopicModeler(method='lda')\n",
    "\n",
    "# Prepare texts for topic modeling\n",
    "texts = df_sentiment['review_text_clean'].tolist()\n",
    "print(f\"Preparing {len(texts)} texts for topic modeling...\")\n",
    "\n",
    "# Find optimal number of topics (simplified for demo)\n",
    "print(\"Finding optimal number of topics...\")\n",
    "try:\n",
    "    coherence_scores = topic_modeler.find_optimal_topics(texts, max_topics=10, step=2)\n",
    "    print(\"Coherence scores:\", coherence_scores)\n",
    "    optimal_topics = max(coherence_scores, key=coherence_scores.get)\n",
    "    print(f\"Optimal number of topics: {optimal_topics}\")\n",
    "except Exception as e:\n",
    "    print(f\"Coherence optimization failed: {e}\")\n",
    "    optimal_topics = 5\n",
    "    print(f\"Using default number of topics: {optimal_topics}\")\n",
    "\n",
    "# Fit topic model\n",
    "print(f\"Fitting topic model with {optimal_topics} topics...\")\n",
    "topic_modeler.fit_topic_model(texts, num_topics=optimal_topics)\n",
    "\n",
    "# Get topic words\n",
    "topic_words = topic_modeler.get_topic_words(num_words=10)\n",
    "print(\"\\nTopic words:\")\n",
    "for i, words in enumerate(topic_words):\n",
    "    print(f\"Topic {i}: {', '.join(words)}\")\n",
    "\n",
    "# Create topic summary\n",
    "topic_summary = topic_modeler.create_topic_summary(texts)\n",
    "print(\"\\nTopic summary:\")\n",
    "display(topic_summary[['topic_id', 'top_words', 'document_count', 'percentage']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualization generator\n",
    "print(\"Initializing visualization generator...\")\n",
    "viz_generator = RetailVisualizationGenerator()\n",
    "\n",
    "# Create figures directory\n",
    "os.makedirs('../figures', exist_ok=True)\n",
    "\n",
    "print(\"Generating visualizations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sentiment distribution plot\n",
    "print(\"Creating sentiment distribution plot...\")\n",
    "fig1 = viz_generator.plot_sentiment_distribution(\n",
    "    df_sentiment, \n",
    "    sentiment_column='sentiment_sentiment',\n",
    "    save_path='../figures/sentiment_distribution.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sentiment by category plot\n",
    "print(\"Creating sentiment by category plot...\")\n",
    "fig2 = viz_generator.plot_sentiment_by_category(\n",
    "    df_sentiment,\n",
    "    save_path='../figures/sentiment_by_category.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Topic distribution plot\n",
    "print(\"Creating topic distribution plot...\")\n",
    "fig3 = viz_generator.plot_topic_distribution(\n",
    "    topic_summary,\n",
    "    save_path='../figures/topic_distribution.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sentiment confidence distribution\n",
    "print(\"Creating sentiment confidence plot...\")\n",
    "fig4 = viz_generator.plot_sentiment_confidence(\n",
    "    df_sentiment,\n",
    "    save_path='../figures/sentiment_confidence.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Word Cloud Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word cloud for all reviews\n",
    "print(\"Creating word cloud for all reviews...\")\n",
    "all_text = ' '.join(df_sentiment['review_text_clean'].tolist())\n",
    "wordcloud = viz_generator.create_word_cloud(\n",
    "    all_text, \n",
    "    title=\"Customer Reviews Word Cloud\",\n",
    "    save_path='../figures/wordcloud_all.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for each sentiment\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    if sentiment in df_sentiment['sentiment_sentiment'].values:\n",
    "        sentiment_text = ' '.join(\n",
    "            df_sentiment[df_sentiment['sentiment_sentiment'] == sentiment]['review_text_clean'].tolist()\n",
    "        )\n",
    "        if sentiment_text.strip():\n",
    "            print(f\"Creating word cloud for {sentiment} reviews...\")\n",
    "            wordcloud = viz_generator.create_word_cloud(\n",
    "                sentiment_text, \n",
    "                title=f\"{sentiment.capitalize()} Reviews Word Cloud\",\n",
    "                save_path=f'../figures/wordcloud_{sentiment}.png'\n",
    "            )\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive dashboard\n",
    "print(\"Creating comprehensive dashboard...\")\n",
    "dashboard = viz_generator.create_comprehensive_dashboard(\n",
    "    df_sentiment, \n",
    "    topic_summary,\n",
    "    save_path='../figures/comprehensive_dashboard.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Business Intelligence Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate business insights\n",
    "print(\"Generating business intelligence insights...\")\n",
    "\n",
    "insights = {\n",
    "    'total_reviews': len(df_sentiment),\n",
    "    'sentiment_distribution': df_sentiment['sentiment_sentiment'].value_counts().to_dict(),\n",
    "    'avg_confidence': df_sentiment['sentiment_confidence'].mean(),\n",
    "    'category_performance': df_sentiment.groupby('product_category')['sentiment_sentiment'].value_counts().to_dict(),\n",
    "    'top_topics': topic_summary[['topic_id', 'top_words', 'percentage']].to_dict('records'),\n",
    "    'rating_sentiment_correlation': df_sentiment.groupby('rating')['sentiment_sentiment'].value_counts().to_dict()\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"BUSINESS INTELLIGENCE INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Reviews Analyzed: {insights['total_reviews']:,}\")\n",
    "print(f\"Average Sentiment Confidence: {insights['avg_confidence']:.3f}\")\n",
    "\n",
    "print(\"\\n📊 SENTIMENT DISTRIBUTION:\")\n",
    "for sentiment, count in insights['sentiment_distribution'].items():\n",
    "    percentage = (count / insights['total_reviews']) * 100\n",
    "    print(f\"  {sentiment.capitalize()}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n🏷️ TOP TOPICS:\")\n",
    "for topic in insights['top_topics']:\n",
    "    print(f\"  Topic {topic['topic_id']}: {topic['top_words'][:50]}... ({topic['percentage']:.1f}%)\")\n",
    "\n",
    "print(\"\\n📈 CATEGORY PERFORMANCE:\")\n",
    "category_sentiment = df_sentiment.groupby(['product_category', 'sentiment_sentiment']).size().unstack(fill_value=0)\n",
    "category_sentiment_pct = category_sentiment.div(category_sentiment.sum(axis=1), axis=0) * 100\n",
    "display(category_sentiment_pct.round(1))\n",
    "\n",
    "print(\"\\n⭐ RATING vs SENTIMENT CORRELATION:\")\n",
    "rating_sentiment = pd.crosstab(df_sentiment['rating'], df_sentiment['sentiment_sentiment'], normalize='index') * 100\n",
    "display(rating_sentiment.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Actionable Business Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate actionable recommendations\n",
    "print(\"=\" * 50)\n",
    "print(\"ACTIONABLE BUSINESS RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate key metrics for recommendations\n",
    "positive_pct = (insights['sentiment_distribution'].get('positive', 0) / insights['total_reviews']) * 100\n",
    "negative_pct = (insights['sentiment_distribution'].get('negative', 0) / insights['total_reviews']) * 100\n",
    "\n",
    "# Category with lowest positive sentiment\n",
    "category_positive = df_sentiment[df_sentiment['sentiment_sentiment'] == 'positive']['product_category'].value_counts(normalize=True) * 100\n",
    "category_negative = df_sentiment[df_sentiment['sentiment_sentiment'] == 'negative']['product_category'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"\\n🎯 IMMEDIATE ACTIONS (0-3 months):\")\n",
    "print(f\"1. Monitor negative sentiment ({negative_pct:.1f}%) - implement real-time alerts\")\n",
    "print(f\"2. Address quality issues in categories with high negative sentiment\")\n",
    "print(f\"3. Leverage positive sentiment ({positive_pct:.1f}%) for marketing campaigns\")\n",
    "print(f\"4. Set up automated sentiment monitoring dashboard\")\n",
    "\n",
    "print(f\"\\n📊 STRATEGIC INITIATIVES (3-12 months):\")\n",
    "print(f\"1. Implement predictive analytics for customer satisfaction\")\n",
    "print(f\"2. Develop category-specific improvement strategies\")\n",
    "print(f\"3. Create sentiment-driven product development roadmap\")\n",
    "print(f\"4. Establish competitive sentiment benchmarking\")\n",
    "\n",
    "print(f\"\\n🚀 LONG-TERM GOALS (12+ months):\")\n",
    "print(f\"1. Achieve 70%+ positive sentiment across all categories\")\n",
    "print(f\"2. Reduce negative sentiment to <15%\")\n",
    "print(f\"3. Implement real-time personalization based on sentiment\")\n",
    "print(f\"4. Establish market leadership in customer satisfaction\")\n",
    "\n",
    "print(f\"\\n💡 INNOVATION OPPORTUNITIES:\")\n",
    "print(f\"1. AI-powered customer service based on sentiment patterns\")\n",
    "print(f\"2. Dynamic pricing models incorporating sentiment data\")\n",
    "print(f\"3. Predictive quality assurance using review analysis\")\n",
    "print(f\"4. Sentiment-driven supply chain optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "print(\"Saving analysis results...\")\n",
    "\n",
    "# Save main results\n",
    "loader.save_processed_data(df_sentiment, 'analyzed_reviews_demo.csv')\n",
    "loader.save_processed_data(topic_summary, 'topic_summary_demo.csv')\n",
    "\n",
    "# Save business insights\n",
    "insights_df = pd.DataFrame({\n",
    "    'metric': ['total_reviews', 'avg_confidence', 'positive_pct', 'negative_pct', 'neutral_pct'],\n",
    "    'value': [\n",
    "        insights['total_reviews'],\n",
    "        insights['avg_confidence'],\n",
    "        (insights['sentiment_distribution'].get('positive', 0) / insights['total_reviews']) * 100,\n",
    "        (insights['sentiment_distribution'].get('negative', 0) / insights['total_reviews']) * 100,\n",
    "        (insights['sentiment_distribution'].get('neutral', 0) / insights['total_reviews']) * 100\n",
    "    ]\n",
    "})\n",
    "loader.save_processed_data(insights_df, 'business_insights_demo.csv')\n",
    "\n",
    "print(\"\\n✅ ANALYSIS COMPLETE!\")\n",
    "print(\"\\n📁 Results saved to:\")\n",
    "print(\"- data/processed/analyzed_reviews_demo.csv\")\n",
    "print(\"- data/processed/topic_summary_demo.csv\")\n",
    "print(\"- data/processed/business_insights_demo.csv\")\n",
    "print(\"- figures/ (all visualization plots)\")\n",
    "\n",
    "print(\"\\n📋 Next steps:\")\n",
    "print(\"1. Review the research paper: paper/research_paper.md\")\n",
    "print(\"2. Explore the generated visualizations in figures/\")\n",
    "print(\"3. Implement recommendations based on business insights\")\n",
    "print(\"4. Set up real-time monitoring using this framework\")\n",
    "\n",
    "print(\"\\n🎉 Thank you for using the Retail Semantic Analysis framework!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
